{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import argparse\n",
    "import math\n",
    "import IPython \n",
    "from PIL import Image\n",
    "from enum import Enum\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "from functools import partial\n",
    "\n",
    "import h5py\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchio as tio\n",
    "import torchvision\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "from scipy import sparse\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt \n",
    "import torchxrayvision as xrv\n",
    "import nibabel as nib\n",
    "from typing import Any, Dict, Optional\n",
    "from collections import OrderedDict\n",
    "from monai.losses.dice import DiceLoss, DiceCELoss\n",
    "\n",
    "from torchmetrics import Metric, MetricCollection\n",
    "from torchmetrics.wrappers import ClasswiseWrapper\n",
    "from torchmetrics.classification import (MultilabelAUROC, MultilabelF1Score, MultilabelAccuracy, MulticlassF1Score, \n",
    "                                        MulticlassAccuracy, MulticlassAUROC, Accuracy, BinaryF1Score, BinaryAUROC,\n",
    "                                        JaccardIndex, MulticlassJaccardIndex, Dice, BinaryAUROC)\n",
    "\n",
    "from fvcore.common.checkpoint import Checkpointer, PeriodicCheckpointer\n",
    "import dinov2.distributed as distributed\n",
    "from dinov2.models.unet import UNet\n",
    "from dinov2.data import SamplerType, make_data_loader, make_dataset\n",
    "from dinov2.data.datasets import NIHChestXray, MC, Shenzhen, SARSCoV2CT, BTCV, BTCVSlice, AMOS, MSDHeart\n",
    "from dinov2.data.datasets.medical_dataset import MedicalVisionDataset\n",
    "from dinov2.data.loaders import make_data_loader\n",
    "from dinov2.data.transforms import (make_segmentation_train_transforms, make_classification_eval_transform, make_segmentation_eval_transforms,\n",
    "                                    make_classification_train_transform)\n",
    "from dinov2.eval.setup import setup_and_build_model\n",
    "from dinov2.eval.utils import (is_padded_matrix, ModelWithIntermediateLayers, ModelWithNormalize, evaluate, extract_features, collate_fn_3d,\n",
    "                               make_datasets, make_data_loaders)\n",
    "from dinov2.eval.classification.utils import LinearClassifier, create_linear_input, setup_linear_classifiers, AllClassifiers\n",
    "from dinov2.eval.metrics import build_segmentation_metrics, MetricAveraging, MetricType\n",
    "from dinov2.eval.segmentation.utils import LinearDecoder, setup_decoders, DINOV2Encoder\n",
    "from dinov2.utils import show_image_from_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def data_split(data_dir=r\"C:\\Users\\Moham\\OneDrive\\Desktop\\MSDSpleen\"):\n",
    "\n",
    "\n",
    "    files = glob.glob(f'{data_dir}/ImagesTr/.*')\n",
    "\n",
    "    for file in files:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {file}: {str(e)}\")\n",
    "\n",
    "    images_path = data_dir + \"\\ImagesTr\" \n",
    "    labels_path = data_dir + \"\\labelsTr\"\n",
    "    os.rename(images_path, data_dir + \"\\images\")\n",
    "    os.rename(labels_path, data_dir + \"\\labels\")\n",
    "\n",
    "    images_path = data_dir + \"\\images\"\n",
    "    labels_path = data_dir + \"\\labels\"\n",
    "\n",
    "    images = os.listdir(images_path)\n",
    "    n_volumes = len(images)\n",
    "\n",
    "    test_list = np.arange(0, n_volumes, n_volumes/15).round().astype(\"int\")\n",
    "    val_list = np.arange(0, n_volumes-15, (n_volumes-15)/10).round().astype(\"int\")\n",
    "\n",
    "    os.makedirs(data_dir + \"/test\", exist_ok=True)\n",
    "    os.makedirs(data_dir + \"/val\", exist_ok=True)\n",
    "\n",
    "    to_path = data_dir + \"/test\"\n",
    "    volumes_to_remove = []\n",
    "    for index in test_list:\n",
    "        shutil.move(images_path+f\"/{images[index]}\", to_path)\n",
    "        volumes_to_remove.append(images[index])\n",
    "    for volume in volumes_to_remove: images.remove(volume)\n",
    "\n",
    "    to_path = data_dir + \"/val\"\n",
    "    volumes_to_remove = []\n",
    "    for index in val_list:\n",
    "        shutil.move(images_path+f\"/{images[index]}\", to_path)\n",
    "        volumes_to_remove.append(images[index])\n",
    "    for volume in volumes_to_remove: images.remove(volume)\n",
    "\n",
    "    os.rename(images_path, data_dir + \"/train\")\n",
    "\n",
    "def slice_it(data_dir=r\"C:\\Users\\Moham\\OneDrive\\Desktop\\MSDSpleen\\\\\"):\n",
    "    train_data_path = data_dir + \"train/\"\n",
    "    train_data = os.listdir(train_data_path)\n",
    "\n",
    "    val_data_path = data_dir + \"val/\"\n",
    "    val_data = os.listdir(val_data_path)\n",
    "\n",
    "    test_data_path = data_dir + \"test/\"\n",
    "    test_data = os.listdir(test_data_path)\n",
    "\n",
    "    labels_data_path = data_dir + \"labels/\"\n",
    "    labels = os.listdir(labels_data_path)\n",
    "\n",
    "    files = glob.glob(f'{labels_data_path}/.*')\n",
    "\n",
    "    for file in files:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {file}: {str(e)}\")\n",
    "\n",
    "    labels = os.listdir(labels_data_path)\n",
    "\n",
    "    for scan in labels:\n",
    "        nifit_scan = nib.load(labels_data_path + scan)\n",
    "        array = nifit_scan.get_fdata()\n",
    "        array = array.transpose(2, 0, 1)\n",
    "        \n",
    "        scan_name = scan.split(\".\")[0]\n",
    "        for i, slice in enumerate(array):\n",
    "            num = str(i).zfill(3)\n",
    "            np.save(labels_data_path + os.sep + scan_name  + f\"_{num}.npy\", slice)\n",
    "        os.remove(labels_data_path + scan)\n",
    "\n",
    "    for scan in train_data:\n",
    "        nifit_scan = nib.load(train_data_path + scan)\n",
    "        array = nifit_scan.get_fdata()\n",
    "        array = array.transpose(2, 0, 1)\n",
    "        \n",
    "        scan_name = scan.split(\".\")[0]\n",
    "        for i, slice in enumerate(array):\n",
    "            num = str(i).zfill(3)\n",
    "            np.save(train_data_path + os.sep + scan_name  + f\"_{num}.npy\", slice)\n",
    "        os.remove(train_data_path + scan)\n",
    "\n",
    "    for scan in val_data:\n",
    "        nifit_scan = nib.load(val_data_path + scan)\n",
    "        array = nifit_scan.get_fdata()\n",
    "        array = array.transpose(2, 0, 1)\n",
    "        \n",
    "        scan_name = scan.split(\".\")[0]\n",
    "        for i, slice in enumerate(array):\n",
    "            num = str(i).zfill(3)\n",
    "            np.save(val_data_path + os.sep + scan_name  + f\"_{num}.npy\", slice)\n",
    "        os.remove(val_data_path + scan)\n",
    "\n",
    "    for scan in test_data:\n",
    "        nifit_scan = nib.load(test_data_path + scan)\n",
    "        array = nifit_scan.get_fdata()\n",
    "        array = array.transpose(2, 0, 1)\n",
    "        \n",
    "        scan_name = scan.split(\".\")[0]\n",
    "        for i, slice in enumerate(array):\n",
    "            num = str(i).zfill(3)\n",
    "            np.save(test_data_path + os.sep + scan_name  + f\"_{num}.npy\", slice)\n",
    "        os.remove(test_data_path + scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "import nibabel as nib\n",
    "slice_it()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Split(Enum):\n",
    "    TRAIN = \"train\"\n",
    "    VAL = \"val\"\n",
    "    TEST = \"test\"\n",
    "\n",
    "    @property\n",
    "    def length(self) -> int:\n",
    "        split_lengths = {\n",
    "            _Split.TRAIN: 191_027,\n",
    "            _Split.VAL: 202,\n",
    "            _Split.TEST: 518,\n",
    "        }\n",
    "        return split_lengths[self]\n",
    "\n",
    "class CheXpert(MedicalVisionDataset):\n",
    "    Split = _Split\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        split: \"CheXpert.Split\",\n",
    "        root: str,\n",
    "        transforms: Optional[Callable] = None,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        root = root + os.sep\n",
    "        super().__init__(split, root, transforms, transform, target_transform)\n",
    "        \n",
    "        # Set the labels dataframe\n",
    "        self.root = root + os.sep\n",
    "        self.labels = pd.read_csv(self.root + self._split.value + \".csv\")\n",
    "        self._clean_labels()\n",
    "\n",
    "    def _check_size(self):\n",
    "        t = pd.read_csv(self.root + self._split.value + \".csv\")\n",
    "        t= t[~t['Path'].str.contains('lateral')].reset_index(drop=True)\n",
    "        num_of_images = len(t)\n",
    "        logger.info(f\"{self._split.length - num_of_images} scans are missing from {self._split.value.upper()} set\")\n",
    "\n",
    "    def _clean_labels(self):\n",
    "\n",
    "        self.labels = self.labels[~self.labels['Path'].str.contains('lateral')].reset_index(drop=True)\n",
    "        self.labels.fillna(0, inplace=True)\n",
    "        self.labels = self.labels[[\"Path\", \"Cardiomegaly\", \"Edema\", \"Consolidation\", \"Atelectasis\", \"Pleural Effusion\"]]\n",
    "        self.labels[\"Uncertain\"] = 0\n",
    "\n",
    "        # Loop through the rows of the table\n",
    "        for i, row in self.labels.iterrows():\n",
    "            # Check if any of the diseases have value -1\n",
    "            if any(row[[\"Cardiomegaly\", \"Edema\", \"Consolidation\", \"Atelectasis\", \"Pleural Effusion\"]] == -1):\n",
    "                # Set uncertain to 1\n",
    "                self.labels.loc[i, \"Uncertain\"] = 1\n",
    "        self.labels.replace(-1, 0, inplace=True)\n",
    "\n",
    "        classes = [\"Cardiomegaly\", \"Edema\", \"Consolidation\", \"Atelectasis\", \"Pleural Effusion\", \"Uncertain\"]\n",
    "        self.targets = self.labels[classes].to_numpy()\n",
    "        self.class_names = classes\n",
    "\n",
    "    @property\n",
    "    def split(self) -> \"CheXpert.Split\":\n",
    "        return self._split\n",
    "    \n",
    "    def get_length(self) -> int:\n",
    "        return self.__len__()\n",
    "    \n",
    "    def get_num_classes(self) -> int:\n",
    "        return len(self.class_names)\n",
    "\n",
    "    def is_3d(self) -> bool:\n",
    "        return False\n",
    "    \n",
    "    def is_multilabel(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def get_image_data(self, index: int) :\n",
    "        data_point = self.labels.iloc[index] \n",
    "        rel = f\"{os.sep}\".join(data_point[\"Path\"].split(f\"{os.sep}\")[1:]) if self._split == _Split.TEST else \\\n",
    "                f\"{os.sep}\".join(data_point[\"Path\"].split(f\"{os.sep}\")[2:])\n",
    "        image_path = self._split_dir + os.sep + rel\n",
    "        \n",
    "        # Read as gray because some of the images have extra layers in the 3rd dimension\n",
    "        image = skimage.io.imread(image_path).astype(np.float16)\n",
    "        image = np.stack((image,)*3, axis=0)\n",
    "        image = torch.from_numpy(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def get_target(self, index: int):\n",
    "        return self.targets[index].astype(np.int64)\n",
    "\n",
    "    def get_targets(self) -> np.ndarray:\n",
    "        return self.targets\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.get_image_data(index)\n",
    "        target = self.get_target(index)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image, target = self.transforms(image, target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = CheXpert(root=\"/mnt/d/data/tmp/CheXpert/\", split=_Split.TRAIN)\n",
    "d.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Assuming 'file_paths' is your list of .npy file paths\n",
    "for file_path in i:\n",
    "    # Load the .npy file\n",
    "    arr = np.load(file_path)\n",
    "\n",
    "    # Assuming 'original_image' is your image\n",
    "    arr = 255 * ((arr - arr.min()) / (arr.max() - arr.min()))\n",
    "\n",
    "    # Convert the numpy array to a PIL Image\n",
    "    img = Image.fromarray(arr)\n",
    "    img = img.convert(\"L\")\n",
    "    \n",
    "    # Save the image\n",
    "    img.save(file_path.replace('.npy', '.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SystemicSampler(torch.utils.data.Subset):\n",
    "    def __init__(self, dataset, num_samples=5000):\n",
    "        dataset_len = dataset.__len__()\n",
    "        indices = np.arange(0, dataset_len, dataset_len/num_samples).round().astype(\"int\")\n",
    "        self.subset = torch.utils.data.Subset(dataset, indices)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        return self.subset.__getitem__(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.subset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "# os.environ['TMPDIR'] = 'D:/data/tmp'\n",
    "\n",
    "def resize_and_save_image(image_path, output_dir):\n",
    "    with Image.open(image_path) as img:\n",
    "        # Resize the image\n",
    "        img = img.resize((1024, 1024))\n",
    "        # Create the same directory structure in the output directory\n",
    "        output_path = os.path.join(output_dir, os.path.basename(image_path))\n",
    "        # os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        # Save the image\n",
    "        print(output_path)\n",
    "        # img.save(output_path)\n",
    "\n",
    "def process_dataset(input_dir, output_dir):\n",
    "    print()\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        print(files)\n",
    "        for file in files:\n",
    "            if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                # Get the full file path\n",
    "                file_path = os.path.join(root, file)\n",
    "                # Resize and save the image\n",
    "                resize_and_save_image(file_path, output_dir)\n",
    "\n",
    "# Call the function\n",
    "process_dataset('D:/data/tmp/CheXpert/test/', 'D:/data/a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/mnt/z/data/amos22/\"\n",
    "train_data_path = data_dir + \"train/\"\n",
    "train_data = os.listdir(train_data_path)\n",
    "\n",
    "val_data_path = data_dir + \"val/\"\n",
    "val_data = os.listdir(val_data_path)\n",
    "\n",
    "test_data_path = data_dir + \"test/\"\n",
    "test_data = os.listdir(test_data_path)\n",
    "\n",
    "label_data_path = data_dir + \"labels/\"\n",
    "label_data = os.listdir(label_data_path)\n",
    "\n",
    "for label in label_data:\n",
    "    if label not in train_data or label not in val_data or label not in test_data:\n",
    "        os.remove(label_data_path + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/mnt/z/data/amos22/\"\n",
    "images_data_path = data_dir + \"images/\"\n",
    "images_data = os.listdir(images_data_path)\n",
    "\n",
    "train_data_path = data_dir + \"train\"\n",
    "val_data_path = data_dir + \"val\"\n",
    "test_data_path = data_dir + \"test\"\n",
    "label_data_path = data_dir + \"labels\"\n",
    "\n",
    "# os.makedirs(train_data_path, exist_ok=True)\n",
    "# train_images = np.arange(0, 300, 300/80).round().astype(\"int\")\n",
    "# os.makedirs(val_data_path, exist_ok=True)\n",
    "# val_images = np.arange(0, 220, 220/20).round().astype(\"int\")\n",
    "# os.makedirs(test_data_path, exist_ok=True)\n",
    "# test_images = np.arange(0, 200, 200/50).round().astype(\"int\")\n",
    "\n",
    "# to_remove = []\n",
    "# for indx in train_images:\n",
    "#     image = images_data[indx]\n",
    "#     image_path = images_data_path + image\n",
    "#     shutil.move(image_path, train_data_path)\n",
    "#     to_remove.append(image)\n",
    "# for img in to_remove: images_data.remove(img)\n",
    "\n",
    "# to_remove = []\n",
    "# for indx in val_images:\n",
    "#     image = images_data[indx]\n",
    "#     image_path = images_data_path + image\n",
    "#     shutil.move(image_path, val_data_path)\n",
    "#     to_remove.append(image)\n",
    "# for img in to_remove: images_data.remove(img)\n",
    "\n",
    "# to_remove = []\n",
    "# for indx in test_images:\n",
    "#     image = images_data[indx]\n",
    "#     image_path = images_data_path + image\n",
    "#     shutil.move(image_path, test_data_path)\n",
    "#     to_remove.append(image)\n",
    "# for img in to_remove: images_data.remove(img)\n",
    "\n",
    "train_data = os.listdir(train_data_path)\n",
    "val_data = os.listdir(val_data_path)\n",
    "test_data = os.listdir(test_data_path)\n",
    "label_data = os.listdir(label_data_path)\n",
    "\n",
    "for scan in label_data:\n",
    "    if scan not in train_data and scan not in val_data and scan not in test_data:\n",
    "        os.remove(label_data_path + os.sep + scan)\n",
    "\n",
    "for scan in train_data:\n",
    "    nifit_scan = nib.load(train_data_path + os.sep + scan)\n",
    "    array = nifit_scan.get_fdata()\n",
    "    array = array.transpose(2, 0, 1)\n",
    "    scan_name = scan.split(\".\")[0]\n",
    "\n",
    "    for i, slice in enumerate(array):\n",
    "        num = str(i).zfill(3)\n",
    "        np.save(train_data_path + os.sep + scan_name  + f\"_{num}.npy\", slice)\n",
    "    os.remove(train_data_path + os.sep + scan)\n",
    "\n",
    "for scan in val_data:\n",
    "    nifit_scan = nib.load(val_data_path + os.sep + scan)\n",
    "    array = nifit_scan.get_fdata()\n",
    "    array = array.transpose(2, 0, 1)\n",
    "    \n",
    "    scan_name = scan.split(\".\")[0]\n",
    "    for i, slice in enumerate(array):\n",
    "        num = str(i).zfill(3)\n",
    "        np.save(val_data_path + os.sep + scan_name  + f\"_{num}.npy\", slice)\n",
    "    os.remove(val_data_path + os.sep + scan)\n",
    "\n",
    "for scan in test_data:\n",
    "    nifit_scan = nib.load(test_data_path + os.sep + scan)\n",
    "    array = nifit_scan.get_fdata()\n",
    "    array = array.transpose(2, 0, 1)\n",
    "    \n",
    "    scan_name = scan.split(\".\")[0]\n",
    "    for i, slice in enumerate(array):\n",
    "        num = str(i).zfill(3)\n",
    "        np.save(test_data_path + os.sep + scan_name  + f\"_{num}.npy\", slice)\n",
    "    os.remove(test_data_path + os.sep + scan)\n",
    "\n",
    "for scan in label_data:\n",
    "    nifit_scan = nib.load(label_data_path + os.sep + scan)\n",
    "    array = nifit_scan.get_fdata()\n",
    "    array = array.transpose(2, 0, 1)\n",
    "    scan_name = scan.split(\".\")[0]\n",
    "\n",
    "    for i, slice in enumerate(array):\n",
    "        num = str(i).zfill(3)\n",
    "        np.save(label_data_path + os.sep + scan_name  + f\"_{num}.npy\", slice)\n",
    "    os.remove(label_data_path + os.sep + scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.nets import SwinUNETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(config_file='dinov2/configs/eval/vits14_pretrain.yaml', pretrained_weights='models/dinov2_vits14_pretrain.pth', output_dir='results/NIH/dinov2_vits14/knn', opts=[], train_dataset_str='NIHChestXray:split=TRAIN:root=/mnt/d/data/NIH', val_dataset_str='NIHChestXray:split=VAL:root=/mnt/d/data/NIH', test_dataset_str='NIHChestXray:split=TEST:root=/mnt/d/data/NIH', nb_knn=[5, 20, 50, 100, 200], temperature=0.07, gather_on_cpu=False, batch_size=8, n_per_class_list=[-1], n_tries=1, ngpus=1, nodes=1, timeout=2800, partition='learnlab', use_volta32=False, comment='', exclude='')\n",
    "# args = argparse.Namespace(config_file='dinov2/configs/eval/vits14_pretrain.yaml', pretrained_weights='models/dinov2_vits14_pretrain.pth', output_dir='results/NIH/dinov2_vits14/knn', opts=[], train_dataset_str='MC:split=TRAIN:root=/mnt/z/data/MC', val_dataset_str='MC:split=VAL:root=/mnt/z/data/MC', test_dataset_str='MC:split=TEST:root=/mnt/z/data/MC', nb_knn=[5, 20, 50, 100, 200], temperature=0.07, gather_on_cpu=False, batch_size=8, n_per_class_list=[-1], n_tries=1, ngpus=1, nodes=1, timeout=2800, partition='learnlab', use_volta32=False, comment='', exclude='')\n",
    "# args = argparse.Namespace(backbone=\"dinov2\", config_file='dinov2/configs/eval/vitl14_pretrain.yaml', pretrained_weights='models/dinov2_vitl14_pretrain.pth', output_dir='results/NIH/dinov2_vitl14/knn', opts=[], train_dataset_str='BTCV:split=TRAIN:root=/mnt/z/data/BTCV', test_dataset_str='BTCV:split=VAL:root=/mnt/z/data/BTCV', nb_knn=[5, 20, 50, 100, 200], temperature=0.07, gather_on_cpu=False, batch_size=8, n_per_class_list=[-1], n_tries=1, ngpus=1, nodes=1, timeout=2800, partition='learnlab', use_volta32=False, comment='', exclude='')\n",
    "# model, autocast_dtype = setup_and_build_model(args)\n",
    "# autocast_ctx = partial(torch.cuda.amp.autocast, enabled=True, dtype=autocast_dtype)\n",
    "# feature_model_with_inter = ModelWithIntermediateLayers(model, 4, autocast_ctx, is_3d=False)\n",
    "# model = ModelWithNormalize(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dinov2.eval.segmentation.utils import UNetDecoder\n",
    "m = UNetDecoder(1536, out_channels=1, image_size=448)\n",
    "decoder = LinearDecoder(\n",
    "    1024, num_classes=1, image_size=448, patch_size=14\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_classifier = LinearClassifier(\n",
    "    1536, use_n_blocks=1, use_avgpool=False, num_classes=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "CLIPModel.from_pretrained('laion/CLIP-ViT-H-14-laion2B-s32B-b79K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "ViTForImageClassification.from_pretrained('openai/clip-vit-large-patch14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dinov2.eval.utils import trainable_parameters\n",
    "tp, ap = trainable_parameters(model)\n",
    "print(f\"LoRA trainable params: {tp} || all params: {ap} || trainable%: {100 * tp / ap:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamModel, SamProcessor\n",
    "from transformers import (\n",
    "    SamVisionConfig,\n",
    "    SamPromptEncoderConfig,\n",
    "    SamMaskDecoderConfig,\n",
    "    SamModel,\n",
    "    SamConfig,\n",
    ")\n",
    "\n",
    "# config = SamConfig(SamVisionConfig(image_size=1024))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-large\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = torch.rand([1, 3, 1024, 1024]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model.get_image_embeddings(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dinov2.eval.utils import bitfit\n",
    "model = bitfit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_total_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_total_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_transform, train_target_transform = make_segmentation_train_transforms(resize_size=224)\n",
    "eval_image_transform, eval_target_transform  = make_segmentation_eval_transforms(resize_size=224)\n",
    "images = BTCVSlice(root=\"/mnt/z/data/BTCVSlice/\", split=_Split.TRAIN, transform=train_image_transform, target_transform=train_target_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vision_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in train_dataset:\n",
    "    print(i.shape)\n",
    "    l = model.get_image_embeddings(i[0].unsqueeze(0))\n",
    "    print(l.shape)\n",
    "    print(np.unique(t))\n",
    "    print(t.max())\n",
    "    show_image_from_tensor(t[6].unsqueeze(0) * 50 )\n",
    "    print(images.get_num_classes())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_image_transform, train_target_transform = make_segmentation_train_transforms(resize_size=224)\n",
    "# eval_image_transform, eval_target_transform  = make_segmentation_eval_transforms(resize_size=224)\n",
    "train_image_transform = make_classification_train_transform()\n",
    "eval_image_transform = make_classification_eval_transform()\n",
    "\n",
    "train_target_transform = eval_target_transform = None\n",
    "\n",
    "val_dataset_str = args.val_dataset_str\n",
    "# val_dataset_str = None\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = make_datasets(train_dataset_str=args.train_dataset_str, val_dataset_str=val_dataset_str,\n",
    "                                                        test_dataset_str=args.test_dataset_str, train_transform=train_image_transform,\n",
    "                                                        eval_transform=eval_image_transform, train_target_transform=train_target_transform,\n",
    "                                                        eval_target_transform=eval_target_transform)\n",
    "is_3d = test_dataset.is_3d()\n",
    "collate_fn=collate_fn_3d if is_3d else None\n",
    "start_iter=1\n",
    "sampler_type = SamplerType.INFINITE\n",
    "# sampler_type = None\n",
    "seed = 0\n",
    "batch_size = 1\n",
    "num_workers = 0\n",
    "train_data_loader, val_data_loader, test_data_loader = make_data_loaders(train_dataset=train_dataset, test_dataset=test_dataset,\n",
    "                                                                        val_dataset=val_dataset, sampler_type=sampler_type, seed=seed,\n",
    "                                                                        start_iter=start_iter, batch_size=batch_size, num_workers=num_workers,\n",
    "                                                                        collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# train_data_loader = make_data_loader(\n",
    "#     dataset=train_dataset,\n",
    "#     batch_size=2,\n",
    "#     num_workers=0,\n",
    "#     shuffle=True,\n",
    "#     seed=0,\n",
    "#     sampler_type=sampler_type,\n",
    "#     sampler_advance=0,\n",
    "#     drop_last=False,\n",
    "#     persistent_workers=False,\n",
    "#     collate_fn=collate_fn_3d if is_3d else None\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# train_data_loader = make_data_loader(\n",
    "#     dataset=images,\n",
    "#     batch_size=4,\n",
    "#     num_workers=0,\n",
    "#     shuffle=True,\n",
    "#     seed=0,\n",
    "#     sampler_type=sampler_type,\n",
    "#     sampler_advance=0,\n",
    "#     drop_last=False,\n",
    "#     persistent_workers=False,\n",
    "#     collate_fn=collate_fn_3d if is_3d else None\n",
    "# )\n",
    "\n",
    "# val_data_loader = make_data_loader(\n",
    "#     dataset=val_dataset,\n",
    "#     batch_size=4,\n",
    "#     num_workers=0,\n",
    "#     shuffle=True,\n",
    "#     seed=0,\n",
    "#     sampler_type=None,\n",
    "#     sampler_advance=0,\n",
    "#     drop_last=False,\n",
    "#     persistent_workers=False,\n",
    "#     collate_fn=collate_fn_3d if is_3d else None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = SystemicSampler(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in a:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model3DWrapper(nn.Module):\n",
    "    def __init__(self, model, per_slice=False) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.per_slice = per_slice\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_outputs = []\n",
    "        for slices in x: \n",
    "            if self.per_slice:\n",
    "                batch_outputs.append(\n",
    "                    torch.stack([self.model(slice_) for slice_ in slices], dim=0).squeeze()\n",
    "                )\n",
    "            else:\n",
    "                batch_outputs.append(\n",
    "                    self.model(slices)\n",
    "                )\n",
    "        return batch_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_dataset.class_names\n",
    "num_of_classes = train_dataset.get_num_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = build_segmentation_metrics(MetricType.SEGMENTATION_METRICS.accuracy_averaging, num_labels=7, labels=labels.tolist())\n",
    "m = m.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1, lbl1 = test_dataset[0]\n",
    "slice1, slice1lbl = img1[130].cuda(non_blocking=True), lbl1[130].cuda(non_blocking=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice1 = slice1.unsqueeze(0)\n",
    "slice1lbl = slice1lbl.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice1 = slice1.type(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_model = DINOV2Encoder(model, autocast_ctx=autocast_ctx, n_last_blocks=1, is_3d=False).cuda()\n",
    "decoder = LinearDecoder(in_channels=model.embed_dim, num_classes=7, image_size=224).cuda()\n",
    "# decoder = Model3DWrapper(decoder, per_slice=True)\n",
    "optimizer = torch.optim.SGD(decoder.parameters(), lr=3.5e-4, momentum=0.9, weight_decay=0)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "for i in range(1000):\n",
    "    o1 = feature_model(slice1)\n",
    "    o2 = decoder(o1)\n",
    "\n",
    "    \n",
    "    o2 = o2.type(torch.float16)\n",
    "    print(slice1lbl.dtype)\n",
    "    loss = loss_function(o2, slice1lbl)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        # o2 = torch.argmax(o2, dim=1)\n",
    "        print(loss)\n",
    "        # print(m(o2, slice1lbl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dinov2.logging import MetricLogger\n",
    "metric_logger = MetricLogger(delimiter=\"  \")\n",
    "\n",
    "feature_model = DINOV2Encoder(model, autocast_ctx=autocast_ctx, n_last_blocks=1, is_3d=True).cuda()\n",
    "decoder = Model3DWrapper(LinearDecoder(in_channels=model.embed_dim, num_classes=14, image_size=224).cuda(), per_slice=True)\n",
    "        \n",
    "for samples, targets in train_data_loader:\n",
    "\n",
    "    samples = samples.cuda(non_blocking=True)\n",
    "\n",
    "    o1 = feature_model(samples)\n",
    "    # o2 = decoder(o1)\n",
    "\n",
    "    # o2 = torch.cat(o2, dim=0).cuda()\n",
    "    # targets = torch.cat(targets, dim=0).cuda()\n",
    "    # preds = o2.argmax(dim=1)\n",
    "    # targets = targets.type(torch.int64)\n",
    "\n",
    "    # print(o2.shape)\n",
    "    # print(targets.shape)\n",
    "\n",
    "    # z = {\n",
    "    #     \"preds\": preds[50:80],\n",
    "    #     \"target\": targets[50:80],\n",
    "    # }\n",
    "\n",
    "    # print(m.update(**z))\n",
    "    # print(m)\n",
    "    # print(m.compute())\n",
    "\n",
    "    break \n",
    "    # print(t.shape)\n",
    "    # print(t.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = model.embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDecoderUpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, embed_dim=1024) -> None:\n",
    "        super().__init__()\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(out_channels*2, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.skip_conv = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )        \n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.upconv(x1)\n",
    "        x2 = self.skip_conv(x2)\n",
    "        scale_factor = (x1.size()[2] / x2.size()[2])\n",
    "        x2 = nn.Upsample(scale_factor=scale_factor, mode=\"bilinear\", align_corners=True)(x2)\n",
    "        x = torch.concat([x1, x2], dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, image_size=224):\n",
    "        super(UNetDecoder, self).__init__()\n",
    "        self.embed_dim = in_channels\n",
    "        self.image_size = image_size\n",
    "        self.out_channels = out_channels\n",
    "        self.up1 = UNetDecoderUpBlock(in_channels=in_channels, out_channels=in_channels//2, embed_dim=embed_dim)\n",
    "        self.up2 = UNetDecoderUpBlock(in_channels=in_channels//2, out_channels=in_channels//4, embed_dim=embed_dim)\n",
    "        self.up3 = UNetDecoderUpBlock(in_channels=in_channels//4, out_channels=in_channels//8, embed_dim=embed_dim)\n",
    "        self.up4 = UNetDecoderUpBlock(in_channels=in_channels//8, out_channels=out_channels, embed_dim=embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        h = w = self.image_size//14\n",
    "\n",
    "        skip1 = x[3].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        skip2 = x[2].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        skip3 = x[1].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        skip4 = x[0].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        x1    = x[3].reshape(-1, h, w, self.embed_dim).permute(0,3,1,2)\n",
    "        \n",
    "        x2 = self.up1(x1, skip1)\n",
    "        x3 = self.up2(x2, skip2)\n",
    "        x4 = self.up3(x3, skip3)\n",
    "        x5 = self.up4(x4, skip4)\n",
    "\n",
    "        return x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in train_data_loader:\n",
    "    print(i.shape)\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_model = DINOV2Encoder(model, autocast_ctx=autocast_ctx, n_last_blocks=1, is_3d=True).cuda()\n",
    "decoder = LinearDecoder(in_channels=model.embed_dim, num_classes=14, is_3d=True, image_size=224).cuda()\n",
    "# feature_model.eval()\n",
    "# decoder = UNetDecoder(in_channels=model.embed_dim, out_channels=3).cuda()\n",
    "# feature_model_with_inter = ModelWithIntermediateLayers(model, 1, autocast_ctx, is_3d=False)\n",
    "# feature_model_with_inter.eval()\n",
    "for i, t in train_data_loader:\n",
    "    i = i.cuda()\n",
    "    print(i.shape)\n",
    "    embeddings = feature_model(i)\n",
    "    output = decoder(embeddings)\n",
    "    output = torch.stack(output, dim=0)\n",
    "    t = torch.stack(t, dim=0)\n",
    "    # print(t.unsqueeze(1).shape)\n",
    "    print(output.shape)\n",
    "    print(t.shape)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = test_dataset.get_image_data(0)\n",
    "lbl = test_dataset.get_target(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOV2Encoder(torch.nn.Module):\n",
    "    def __init__(self, encoder, autocast_ctx, is_3d=False) -> None:\n",
    "        super(DINOV2Encoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.encoder.eval()\n",
    "        self.autocast_ctx = autocast_ctx\n",
    "        self.is_3d = is_3d\n",
    "    \n",
    "    def forward_3d(self, x):\n",
    "        batch_features = [] \n",
    "        for batch_scans in x: # calculate the features for every scan in all scans of the batch\n",
    "            scans = []\n",
    "            for scan in batch_scans:\n",
    "                if not is_zero_matrix(scan): scans.append(self.forward_(scan.unsqueeze(0)))\n",
    "            batch_features.append(scans)\n",
    "        return batch_features\n",
    "\n",
    "    def forward_(self, x):\n",
    "        with torch.no_grad():\n",
    "            with self.autocast_ctx():\n",
    "                features = self.encoder.forward_features(x)['x_norm_patchtokens']\n",
    "        return features\n",
    "\n",
    "    def forward(self, x):\n",
    "        if is_3d:\n",
    "            return self.forward_3d(x)\n",
    "        return self.forward_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in test_dataset:\n",
    "    show_image_from_tensor(i[0] * 100)\n",
    "    show_image_from_tensor(i[1] * 100)\n",
    "    show_image_from_tensor(i[2] * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_results(feature_model, decoder, dataset):\n",
    "    for i, (img, _) in enumerate(dataset):\n",
    "\n",
    "        img_name = test_dataset.images[i]\n",
    "        _, affine_matrix = test_dataset.get_image_data(i, return_affine_matrix=True)\n",
    "\n",
    "        img = img.cuda(non_blocking=True) \n",
    "\n",
    "        features = feature_model(img.unsqueeze(0))\n",
    "        output = decoder(features, up_size=512)[0]\n",
    "        output = output.argmax(dim=1)\n",
    "\n",
    "        nifti_img = nib.Nifti1Image(output\n",
    "                                    .cpu()\n",
    "                                    .numpy()\n",
    "                                    .astype(np.uint8)\n",
    "                                    .transpose(1, 2, 0), affine_matrix)    \n",
    "        file_output_dir = test_results_path + os.sep + img_name + \".gz\"\n",
    "\n",
    "        # Save the NIfTI image\n",
    "        nib.save(nifti_img, file_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = DINOV2Encoder(model, autocast_ctx=autocast_ctx, is_3d=True).cuda()\n",
    "ld = LinearDecoder(in_channels=model.embed_dim, num_classes=14, is_3d=True).cuda()\n",
    "save_test_results(f, ld, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = DINOV2Encoder(model, autocast_ctx=autocast_ctx, is_3d=True).cuda()\n",
    "ld = LinearDecoder(in_channels=model.embed_dim, num_classes=14, is_3d=True).cuda()\n",
    "optimizer = torch.optim.SGD(ld.parameters(), lr=3e-4, momentum=0.9, weight_decay=0)\n",
    "\n",
    "for i, t in train_data_loader:\n",
    "    i = i.cuda(non_blocking=True) \n",
    "\n",
    "    features = f(i)\n",
    "    output = ld(features)\n",
    "    \n",
    "    output = torch.cat(output, dim=0)\n",
    "    t = torch.cat(t, dim=0)\n",
    "\n",
    "    loss = nn.CrossEntropyLoss()(output, t.cuda(non_blocking=True).type(torch.int64))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # step\n",
    "    optimizer.step()\n",
    "    # labels = t.view(-1, t.shape[-1], t.shape[-1])\n",
    "    # losses = nn.CrossEntropyLoss()(output.view(-1, 14, labels.shape[-1], labels.shape[-1]), labels)\n",
    "        \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ld = LinearDecoder(in_channels=embed_dim, num_classes=3, is_3d=True)\n",
    "ld = ld.cuda()\n",
    "\n",
    "o = ld(features)\n",
    "print(len(o))\n",
    "print(o.shape)\n",
    "o = torch.stack([torch.nn.functional.interpolate(batch_output, size=448, mode=\"bilinear\", align_corners=False)\n",
    "                for batch_output in torch.unbind(o, dim=0)], dim=0)\n",
    "# ou = torch.nn.functional.interpolate(o[0], size=448, mode=\"bilinear\", align_corners=False)\n",
    "print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in train_data_loader:\n",
    "    i = i.cuda()\n",
    "    i = feature_model(i)\n",
    "    print(len(i))\n",
    "    print(len(i[0]))\n",
    "    print(len(i[0][0]))\n",
    "    print(len(i[0][0][0]))\n",
    "    print(len(i[0][0][0][0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDecoder(torch.nn.Module):\n",
    "    \"\"\"Linear decoder head\"\"\"\n",
    "    DECODER_TYPE = \"linear\"\n",
    "\n",
    "    def __init__(self, in_channels, tokenW=32, tokenH=32, num_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.width = tokenW\n",
    "        self.height = tokenH\n",
    "        self.decoder = torch.nn.Conv2d(in_channels, num_classes, (1,1))\n",
    "        self.decoder.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.decoder.bias.data.zero_()\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        print(embeddings.shape)\n",
    "        embeddings = embeddings.reshape(-1, self.height, self.width, self.in_channels)\n",
    "        print(embeddings.shape)\n",
    "        embeddings = embeddings.permute(0,3,1,2)\n",
    "        print(embeddings.shape)\n",
    "\n",
    "        return self.decoder(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = LinearDecoder(384, num_classes=2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in train_dataset:\n",
    "    i = i.cuda().unsqueeze(0)\n",
    "    a = model(i)\n",
    "    b = model.forward_features(i)['x_norm_patchtokens']\n",
    "    z = d(b)\n",
    "    print(z.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concated = torch.utils.data.ConcatDataset([train_dataset, val_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concated.get_num_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in concated:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/mnt/d/data/NIH/\"\n",
    "train_val = pd.read_csv(data_dir + os.sep + \"train_val_list.txt\", names=[\"Image Index\"])\n",
    "val_list = [i for i in range(len(train_val)-10_002, len(train_val))]\n",
    "val_set = train_val.iloc[val_list]\n",
    "train_set = train_val.drop(val_list)\n",
    "\n",
    "train_dir = data_dir + os.sep + \"train\"\n",
    "val_dir = data_dir + os.sep + \"val\"\n",
    "for image in val_set[\"Image Index\"]:\n",
    "    source = train_dir + os.sep + image\n",
    "    dest = val_dir + os.sep + image\n",
    "    shutil.move(source, dest)\n",
    "\n",
    "val_set.to_csv(data_dir + os.sep + \"val_list.txt\", index=False, header=False)\n",
    "train_set.to_csv(data_dir + os.sep + \"train_list.txt\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDecoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, tokenW=32, tokenH=32, num_labels=1):\n",
    "        super(LinearDecoder, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.width = tokenW\n",
    "        self.height = tokenH\n",
    "        self.decoder = torch.nn.Conv2d(in_channels, num_labels, (1,1))\n",
    "        self.decoder.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.decoder.bias.data.zero_()\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        embeddings = embeddings.reshape(-1, self.height, self.width, self.in_channels)\n",
    "        embeddings = embeddings.permute(0,3,1,2)\n",
    "\n",
    "        return self.decoder(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = LinearDecoder(384, num_labels=3).cuda()\n",
    "optimizer = torch.optim.SGD(params=decoder.parameters(), lr=0.0005, momentum=0.9, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 69, eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricAveraging(Enum):\n",
    "    MEAN_ACCURACY = \"micro\"\n",
    "    MEAN_PER_CLASS_ACCURACY = \"macro\"\n",
    "    MULTILABEL_ACCURACY = \"macro\"\n",
    "    MULTILABEL_AUROC = \"macro\"\n",
    "    MULTILABEL_JACCARD = \"macro\"\n",
    "    PER_CLASS_ACCURACY = \"none\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "\n",
    "metric = build_segmentation_metrics(average_type=MetricAveraging.MULTILABEL_JACCARD,num_labels=3)\n",
    "metric.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for image, target in train_data_loader:\n",
    "    i+=1\n",
    "    image, target = image.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
    "    with torch.no_grad(): \n",
    "        features=model.forward_features(image)['x_norm_patchtokens']\n",
    "    logits = decoder(features)\n",
    "    logits = torch.nn.functional.interpolate(logits, size=448, mode=\"bilinear\", align_corners=False)\n",
    "    prediction = logits.argmax(dim=1)\n",
    "\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fct(logits, target)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    metric(prediction, target)\n",
    "    print(metric.compute())\n",
    "    print(loss.item())\n",
    "\n",
    "    # if i % 50 == 0:\n",
    "    show_image_from_tensor((prediction * 100).cpu())\n",
    "    show_image_from_tensor((target * 100).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
