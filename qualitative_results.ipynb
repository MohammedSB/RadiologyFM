{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import argparse\n",
    "import IPython \n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "from enum import Enum\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt \n",
    "import torchxrayvision as xrv\n",
    "\n",
    "\n",
    "from fvcore.common.checkpoint import Checkpointer, PeriodicCheckpointer\n",
    "import dinov2.distributed as distributed\n",
    "from dinov2.eval.metrics import MetricAveraging, build_metric, build_segmentation_metrics\n",
    "from dinov2.models.unet import UNet\n",
    "from dinov2.data import SamplerType, make_data_loader, make_dataset\n",
    "from dinov2.data.datasets import NIHChestXray, MC, Shenzhen, SARSCoV2CT\n",
    "from dinov2.data.datasets.medical_dataset import MedicalVisionDataset\n",
    "from dinov2.data.loaders import make_data_loader\n",
    "from dinov2.data.transforms import (make_segmentation_train_transforms, make_classification_eval_transform, make_segmentation_eval_transforms,\n",
    "                                    make_classification_train_transform)\n",
    "from dinov2.eval.setup import setup_and_build_model\n",
    "from dinov2.eval.utils import (is_padded_matrix, ModelWithIntermediateLayers, ModelWithNormalize, evaluate, extract_features, collate_fn_3d,\n",
    "                               make_datasets)\n",
    "from dinov2.eval.classification.utils import LinearClassifier, create_linear_input, setup_linear_classifiers, AllClassifiers\n",
    "from dinov2.eval.metrics import build_segmentation_metrics\n",
    "from dinov2.eval.segmentation.utils import LinearDecoder, setup_decoders, DINOV2Encoder\n",
    "from dinov2.utils import show_image_from_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = argparse.AMOSace(backbone=\"dinov2\", config_file='dinov2/configs/eval/vitl14_pretrain.yaml', pretrained_weights='/home/baharoon/models/dinov2_vitl14_pretrain.pth', output_dir='results/test', opts=[], train_dataset_str='NIHChestXray:split=TRAIN:root=/home/baharoon/Data/NIH', val_dataset_str='NIHChestXray:split=VAL:root=/home/baharoon/Data/NIH', test_dataset_str='NIHChestXray:split=TEST:root=/home/baharoon/Data/NIH', nb_knn=[5, 20, 50, 100, 200], temperature=0.07, gather_on_cpu=False, batch_size=8, n_per_class_list=[-1], n_tries=1, ngpus=1, nodes=1, timeout=2800, partition='learnlab', use_volta32=False, comment='', exclude='')\n",
    "args = argparse.Namespace(backbone=\"dinov2\", config_file='dinov2/configs/eval/vitl14_pretrain.yaml', pretrained_weights='/home/baharoon/models/dinov2_vitl14_pretrain.pth', output_dir='results/test', opts=[], train_dataset_str='MC:split=TRAIN:root=/home/baharoon/Data/MC', val_dataset_str='MC:split=VAL:root=/home/baharoon/Data/MC', test_dataset_str='MSDHeart:split=TEST:root=/home/baharoon/Data/MSDHeart', nb_knn=[5, 20, 50, 100, 200], temperature=0.07, gather_on_cpu=False, batch_size=8, n_per_class_list=[-1], n_tries=1, ngpus=1, nodes=1, timeout=2800, partition='learnlab', use_volta32=False, comment='', exclude='')\n",
    "model, autocast_dtype = setup_and_build_model(args)\n",
    "autocast_ctx = partial(torch.cuda.amp.autocast, enabled=True, dtype=autocast_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/baharoon/Data/MSDHeart/test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/user/Desktop/dinov2/qualitative_results.ipynb Cell 3\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/qualitative_results.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m eval_image_transform, eval_target_transform  \u001b[39m=\u001b[39m make_segmentation_eval_transforms()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/qualitative_results.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m resize \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mResize((\u001b[39m448\u001b[39m, \u001b[39m448\u001b[39m), interpolation\u001b[39m=\u001b[39mtransforms\u001b[39m.\u001b[39mInterpolationMode\u001b[39m.\u001b[39mBICUBIC)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/qualitative_results.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m test_transformed \u001b[39m=\u001b[39m make_dataset(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/qualitative_results.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     dataset_str\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mtest_dataset_str,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/qualitative_results.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     transform\u001b[39m=\u001b[39;49meval_image_transform,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/qualitative_results.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     target_transform\u001b[39m=\u001b[39;49meval_target_transform,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/qualitative_results.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/qualitative_results.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m test \u001b[39m=\u001b[39m make_dataset(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/qualitative_results.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     dataset_str\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mtest_dataset_str,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/user/Desktop/dinov2/qualitative_results.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m )\n",
      "File \u001b[0;32m/mnt/c/Users/user/Desktop/dinov2/dinov2/data/loaders.py:110\u001b[0m, in \u001b[0;36mmake_dataset\u001b[0;34m(dataset_str, transform, target_transform)\u001b[0m\n\u001b[1;32m    107\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39musing dataset: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdataset_str\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    109\u001b[0m class_, kwargs \u001b[39m=\u001b[39m _parse_dataset_str(dataset_str)\n\u001b[0;32m--> 110\u001b[0m dataset \u001b[39m=\u001b[39m class_(transform\u001b[39m=\u001b[39;49mtransform, target_transform\u001b[39m=\u001b[39;49mtarget_transform, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    112\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m# of dataset samples: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(dataset)\u001b[39m:\u001b[39;00m\u001b[39m,d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[39m# Aggregated datasets do not expose (yet) these attributes, so add them.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/user/Desktop/dinov2/dinov2/data/datasets/msd_heart.py:44\u001b[0m, in \u001b[0;36mMSDHeart.__init__\u001b[0;34m(self, split, root, transforms, transform, target_transform)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     36\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     37\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m     target_transform: Optional[Callable] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     43\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(split, root, transforms, transform, target_transform)\n\u001b[1;32m     46\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimages \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msort(np\u001b[39m.\u001b[39marray(os\u001b[39m.\u001b[39mlistdir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_dir)))\n\u001b[1;32m     48\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_labels_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mos\u001b[39m.\u001b[39msep\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_dir\u001b[39m.\u001b[39msplit(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mos\u001b[39m.\u001b[39msep\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]) \u001b[39m+\u001b[39m os\u001b[39m.\u001b[39msep \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/mnt/c/Users/user/Desktop/dinov2/dinov2/data/datasets/medical_dataset.py:24\u001b[0m, in \u001b[0;36mMedicalVisionDataset.__init__\u001b[0;34m(self, split, root, transforms, transform, target_transform)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split \u001b[39m=\u001b[39m split\n\u001b[1;32m     23\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_define_split_dir()\n\u001b[0;32m---> 24\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_size()\n\u001b[1;32m     26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimages \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msort(np\u001b[39m.\u001b[39marray(os\u001b[39m.\u001b[39mlistdir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_dir)))\n",
      "File \u001b[0;32m/mnt/c/Users/user/Desktop/dinov2/dinov2/data/datasets/medical_dataset.py:36\u001b[0m, in \u001b[0;36mMedicalVisionDataset._check_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_size\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 36\u001b[0m     num_of_images \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(os\u001b[39m.\u001b[39;49mlistdir(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_split_dir))\n\u001b[1;32m     37\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split\u001b[39m.\u001b[39mlength\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mnum_of_images\u001b[39m}\u001b[39;00m\u001b[39m scans are missing from \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split\u001b[39m.\u001b[39mvalue\u001b[39m.\u001b[39mupper()\u001b[39m}\u001b[39;00m\u001b[39m set\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/baharoon/Data/MSDHeart/test'"
     ]
    }
   ],
   "source": [
    "train_image_transform, train_target_transform = make_segmentation_train_transforms()\n",
    "eval_image_transform, eval_target_transform  = make_segmentation_eval_transforms()\n",
    "\n",
    "resize = transforms.Resize((448, 448), interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "\n",
    "test_transformed = make_dataset(\n",
    "    dataset_str=args.test_dataset_str,\n",
    "    transform=eval_image_transform,\n",
    "    target_transform=eval_target_transform,\n",
    ")\n",
    "\n",
    "test = make_dataset(\n",
    "    dataset_str=args.test_dataset_str,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.01]\n",
    "num_of_classes = test.get_num_classes()\n",
    "is_3d = test.is_3d()\n",
    "embed_dim = model.embed_dim\n",
    "decoder_type = \"linear\"\n",
    "resize_size = 448\n",
    "decoders, optim_param_groups = setup_decoders(\n",
    "    embed_dim,\n",
    "    learning_rates,\n",
    "    num_of_classes,\n",
    "    decoder_type,\n",
    "    is_3d=is_3d,\n",
    "    image_size=resize_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/home/baharoon/dinov2/results/mcdinov2vitllinear/optimal/model_final.pth\"\n",
    "checkpointer = Checkpointer(decoders, output_dir)\n",
    "start_iter = checkpointer.resume_or_load(output_dir, resume=True).get(\"iteration\", -1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocast_ctx = partial(torch.cuda.amp.autocast, enabled=True, dtype=autocast_dtype)\n",
    "decoder = list(decoders.module.decoders_dict.values())[0]\n",
    "feature_model = DINOV2Encoder(model, autocast_ctx=autocast_ctx)\n",
    "output_dir = \"/home/baharoon/dinov2/results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_multipler = 255//num_of_classes\n",
    "metric = build_segmentation_metrics(average_type=MetricAveraging.SEGMENTATION_METRICS, num_labels=num_of_classes).cuda()\n",
    "font = ImageFont.truetype(\"/usr/share/fonts/truetype/freefont/FreeMono.ttf\", 15)\n",
    "num_of_images = 1\n",
    "for image_index in range(num_of_images):\n",
    "\n",
    "    image, target = test_transformed[image_index]\n",
    "    image, target = image.cuda(non_blocking=True).unsqueeze(0), target.cuda(non_blocking=True).unsqueeze(0)\n",
    "\n",
    "    untransformed_image = resize(test[image_index][0])[0].cuda()\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        features = feature_model(image)\n",
    "    logits = decoder(features)\n",
    "    logits = torch.nn.functional.interpolate(logits, size=resize_size, mode=\"bilinear\", align_corners=False)\n",
    "    prediction = logits.argmax(dim=1)\n",
    "\n",
    "    results = metric(prediction, target)\n",
    "\n",
    "    prediction = prediction.squeeze()\n",
    "    prediction = (prediction * highlight_multipler).cpu()\n",
    "    H, W = prediction.squeeze().shape\n",
    "    pil_image = torchvision.transforms.ToPILImage()(prediction.type(torch.int32))\n",
    "    pil_image = pil_image.convert(\"L\") # Convert to Grayscale\n",
    "    \n",
    "    draw = ImageDraw.Draw(pil_image)\n",
    "\n",
    "    result_meta = \"\"\n",
    "    for m, r in dict(results).items():\n",
    "        result_meta += f\"{m}: {float(r):.3f} \"\n",
    "\n",
    "    draw.text((0, 0), result_meta, fill=255)\n",
    "\n",
    "    pil_image.save(f\"{output_dir}/{test_transformed.images[image_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
