{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import argparse\n",
    "import math\n",
    "import IPython \n",
    "from PIL import Image\n",
    "from enum import Enum\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "from functools import partial\n",
    "\n",
    "import h5py\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchio as tio\n",
    "import torchvision\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "from scipy import sparse\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt \n",
    "import torchxrayvision as xrv\n",
    "import nibabel as nib\n",
    "from typing import Any, Dict, Optional\n",
    "from collections import OrderedDict\n",
    "from monai.losses.dice import DiceLoss, DiceCELoss\n",
    "\n",
    "from torchmetrics import Metric, MetricCollection\n",
    "from torchmetrics.wrappers import ClasswiseWrapper\n",
    "from torchmetrics.classification import (MultilabelAUROC, MultilabelF1Score, MultilabelAccuracy, MulticlassF1Score, \n",
    "                                        MulticlassAccuracy, MulticlassAUROC, Accuracy, BinaryF1Score, BinaryAUROC,\n",
    "                                        JaccardIndex, MulticlassJaccardIndex, Dice, BinaryAUROC)\n",
    "\n",
    "from fvcore.common.checkpoint import Checkpointer, PeriodicCheckpointer\n",
    "import dinov2.distributed as distributed\n",
    "from dinov2.models.unet import UNet\n",
    "from dinov2.data import SamplerType, make_data_loader, make_dataset\n",
    "from dinov2.data.datasets import NIHChestXray, MC, Shenzhen, SARSCoV2CT, BTCV, BTCVSlice, AMOS, MSDHeart\n",
    "from dinov2.data.datasets.medical_dataset import MedicalVisionDataset\n",
    "from dinov2.data.loaders import make_data_loader\n",
    "from dinov2.data.transforms import (make_segmentation_train_transforms, make_classification_eval_transform, make_segmentation_eval_transforms,\n",
    "                                    make_classification_train_transform)\n",
    "from dinov2.eval.setup import setup_and_build_model\n",
    "from dinov2.eval.utils import (is_padded_matrix, ModelWithIntermediateLayers, ModelWithNormalize, evaluate, extract_features, collate_fn_3d,\n",
    "                               make_datasets, make_data_loaders, apply_method_to_nested_values)\n",
    "from dinov2.eval.classification.utils import LinearClassifier, create_linear_input, setup_linear_classifiers, AllClassifiers\n",
    "from dinov2.eval.metrics import build_segmentation_metrics, MetricAveraging, MetricType\n",
    "from dinov2.eval.segmentation.utils import LinearDecoder, setup_decoders, DINOV2Encoder\n",
    "from dinov2.utils import show_image_from_tensor\n",
    "\n",
    "from dinov2.models.transunet import VisionTransformer as ViT_seg\n",
    "from dinov2.models.transunet import CONFIGS as CONFIGS_ViT_seg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/baharoon/Data/Shenzhen/\"\n",
    "DATA = \"Shenzhen\"\n",
    "OUTPUT_DIR = \"/home/baharoon/dinov2/results/shenzhenunet\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "epochs = 50\n",
    "learning_rates = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1]\n",
    "batch_size = 8\n",
    "sampler_type = None\n",
    "seed = 0\n",
    "\n",
    "image_size = 448\n",
    "train_dataset_str = f\"{DATA}:split=TRAIN:root={DATA_PATH}\"\n",
    "val_dataset_str   = f\"{DATA}:split=VAL:root={DATA_PATH}\"\n",
    "test_dataset_str  = f\"{DATA}:split=TEST:root={DATA_PATH}\"\n",
    "\n",
    "train_image_transform, train_target_transform = make_segmentation_train_transforms(resize_size=image_size)\n",
    "eval_image_transform, eval_target_transform  = make_segmentation_eval_transforms(resize_size=image_size)\n",
    "train_dataset, val_dataset, test_dataset = make_datasets(train_dataset_str=train_dataset_str, val_dataset_str=val_dataset_str,\n",
    "                                                        test_dataset_str=test_dataset_str, train_transform=train_image_transform,\n",
    "                                                        eval_transform=eval_image_transform, train_target_transform=train_target_transform,\n",
    "                                                        eval_target_transform=eval_target_transform)\n",
    "\n",
    "num_of_classes = test_dataset.get_num_classes()\n",
    "\n",
    "\n",
    "epoch_length = math.ceil(len(train_dataset) / batch_size)\n",
    "loss_function = DiceLoss(softmax=True, to_onehot_y=True)\n",
    "\n",
    "train_data_loader, val_data_loader, test_data_loader = make_data_loaders(train_dataset=train_dataset, test_dataset=test_dataset,\n",
    "                                                                        val_dataset=val_dataset, sampler_type=sampler_type, seed=seed,\n",
    "                                                                        start_iter=1, batch_size=batch_size, num_workers=4,\n",
    "                                                                        collate_fn=None)\n",
    "\n",
    "classes = list(test_data_loader.dataset.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_vit = CONFIGS_ViT_seg[\"R50-ViT-L_16\"]\n",
    "config_vit.n_skip = 3\n",
    "config_vit.is_pretrain = False\n",
    "config_vit.n_classes = num_of_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "results = {}\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    model = ViT_seg(config_vit, img_size=448, num_classes=num_of_classes).cuda()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epoch_length * epochs, eta_min=0)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for data, labels in train_data_loader:\n",
    "            data = data.cuda(non_blocking=True)\n",
    "            labels = labels.cuda(non_blocking=True).type(torch.int64)\n",
    "            \n",
    "            output = model(data)\n",
    "\n",
    "            loss = loss_function(output, labels.unsqueeze(1))\n",
    "\n",
    "            # compute the gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # step\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "        if epoch % 10 == 0:\n",
    "            print(loss)\n",
    "            print(optimizer.param_groups[0][\"lr\"])\n",
    "    print(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    metric = build_segmentation_metrics(\n",
    "            average_type=MetricAveraging.SEGMENTATION_METRICS,\n",
    "            num_labels=num_of_classes,\n",
    "            labels=classes\n",
    "        ).cuda()\n",
    "    \n",
    "    for data, labels in val_data_loader:\n",
    "        data = data.cuda(non_blocking=True)\n",
    "        labels = labels.cuda(non_blocking=True).type(torch.int64)\n",
    "\n",
    "        output = model(data)\n",
    "        preds = output.argmax(dim=1)\n",
    "\n",
    "        metric_inputs = {\n",
    "            \"preds\": preds,\n",
    "            \"target\": labels,\n",
    "        }\n",
    "\n",
    "        metric.update(**metric_inputs)\n",
    "\n",
    "    results[f\"{model.__class__.__name__}:lr={learning_rate}\"] = apply_method_to_nested_values(\n",
    "                                                                    metric.compute(),\n",
    "                                                                    method_name=\"item\",\n",
    "                                                                    nested_types=(dict)\n",
    "                                                                    )    \n",
    "print(results)\n",
    "with open(f'{OUTPUT_DIR}/val_result.json', 'w') as f:\n",
    "    # Use json.dump to write dict_data into data.json\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "learning_rate = 1e-3\n",
    "\n",
    "val_dataset = make_dataset(\n",
    "    dataset_str=val_dataset_str,\n",
    "    transform=train_image_transform,\n",
    "    target_transform=train_target_transform\n",
    ")\n",
    "train_dataset = torch.utils.data.ConcatDataset([train_dataset, val_dataset])\n",
    "\n",
    "train_data_loader = make_data_loader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4,\n",
    "    shuffle=True,\n",
    "    seed=seed,\n",
    "    sampler_type=sampler_type,\n",
    "    sampler_advance=1,\n",
    "    drop_last=False,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "model = ViT_seg(config_vit, img_size=448, num_classes=num_of_classes).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epoch_length * epochs, eta_min=0)\n",
    "for epoch in range(epochs):\n",
    "    for data, labels in train_data_loader:\n",
    "        data = data.cuda(non_blocking=True)\n",
    "        labels = labels.cuda(non_blocking=True).type(torch.int64)\n",
    "        \n",
    "        output = model(data)\n",
    "\n",
    "        loss = loss_function(output, labels.unsqueeze(1))\n",
    "\n",
    "        # compute the gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # step\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    if epoch % 50 == 0 and epoch != 0:\n",
    "        metric = build_segmentation_metrics(\n",
    "            average_type=MetricAveraging.SEGMENTATION_METRICS,\n",
    "            num_labels=num_of_classes,\n",
    "            labels=classes\n",
    "        ).cuda()\n",
    "\n",
    "        for data, labels in test_data_loader:\n",
    "            data = data.cuda(non_blocking=True)\n",
    "            labels = labels.cuda(non_blocking=True).type(torch.int64)\n",
    "\n",
    "            output = model(data)\n",
    "            preds = output.argmax(dim=1)\n",
    "\n",
    "            metric_inputs = {\n",
    "                \"preds\": preds,\n",
    "                \"target\": labels,\n",
    "            }\n",
    "\n",
    "            metric.update(**metric_inputs)\n",
    "        print(f\"Epoch: {epoch}, result: {metric.compute()}\")\n",
    "        \n",
    "metric = build_segmentation_metrics(\n",
    "        average_type=MetricAveraging.SEGMENTATION_METRICS,\n",
    "        num_labels=num_of_classes,\n",
    "        labels=classes\n",
    "    ).cuda()\n",
    "\n",
    "for data, labels in test_data_loader:\n",
    "    data = data.cuda(non_blocking=True)\n",
    "    labels = labels.cuda(non_blocking=True).type(torch.int64)\n",
    "\n",
    "    output = model(data)\n",
    "    preds = output.argmax(dim=1)\n",
    "\n",
    "    metric_inputs = {\n",
    "        \"preds\": preds,\n",
    "        \"target\": labels,\n",
    "    }\n",
    "\n",
    "    metric.update(**metric_inputs)\n",
    "\n",
    "results[f\"{model.__class__.__name__}:lr={learning_rate}\"] = apply_method_to_nested_values(\n",
    "                                                                metric.compute(),\n",
    "                                                                method_name=\"item\",\n",
    "                                                                nested_types=(dict)\n",
    "                                                                )    \n",
    "print(results)\n",
    "\n",
    "with open(f'{OUTPUT_DIR}/result.json', 'w') as f:\n",
    "    # Use json.dump to write dict_data into data.json\n",
    "    json.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
